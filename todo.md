# Done
* [03 Mar 20, ND] Scaling of data: Neglect means for SDCAN
* [10 Jan 20, ND] Noisy Hadamard
* [22 Nov 20, ND] Create DCAN with Hadamard patterns
* [22 Nov 20, ND] rename model_Stat_DCAN to model_Had_DCAN

# Todo (improved coding)
* In `model_Had_DCAN` defined forward_* functions and subsequend in `hadNet`
and overload in `noiHadNet`
* Remove (in `nets.py`) `count_parameters`, which is the same as 'count_trainable_param'
* Create folder structure at start/create folder when missing (using pathlib)
* Address warning. "UserWarning: Detected call of `lr_scheduler.step()` before 
`optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the 
opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do
this will result in PyTorch skipping the first value of the learning rate 
schedule. See more details at 
https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
* Counting batch and epochs from 1 (not from 0)
* Suppress Cov and Average txt format generated by Stat_had 
* Replace `sys.path.append('../..')` by something more general? I would like to
run (absolute vs relative path)
* Create a class for data completion?
* Clean up code?

# Todo (issue)
* Second coefficient of measurement vector is negative, which produces
`nan`'s when noise is applied


# Todo (improved method)
* Implement denoised Hadamard network (non trainable, then trainanble)
* Check variance across hadamard coefficient to infer scaling?
* Use same input image several times during training (check robustness to different `N0`)
* Use `N0.f` as input? i.e. include `N0` in `transform`
* Superresolution (inception module) as suggested by Fabien

# Open Questions
* `basic_Hadamard`: in `stat_completion`, `model` -> `net`?
* Have you tested/used basic_Hadamard functions?
* `basic_Hadamard` functions seems redundant with the function I created in 
    `model_Had_DCAN`, especially as those function applies to most of the model's architectures.
* `model_Had_DCAN`: Why not torch.distributions.poisson.Poisson as in basic_Hadamard ?
* Why relu in aquisition block of `hadNet`?
* split returns `T`, sparse matrix seems to be handled as full? Inefficient
* Why `torch.reshape` not torch view in `model_Had_DCAN` ? 
* Implement completion with `conv2D` instead of `Linear`?

# Solved Questions
* Use `conv2D` or `Linear` for scalar products?
    - `Linear` is more efficient for CPUs
    - `conv2D` is more efficient for GPUs
* How to continue training from checkpoint? How to use checkpoints?
    - Just provide full path filename